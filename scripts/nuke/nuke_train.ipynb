{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root:INFO]:starting (cellevents.py:36, time=12:22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16.8 s\n"
     ]
    }
   ],
   "source": [
    "from maskmm.ipstartup import *\n",
    "import os\n",
    "import glob\n",
    "from os.path import join\n",
    "import random\n",
    "import torch\n",
    "from torch import load\n",
    "\n",
    "import skimage.io\n",
    "from skimage.io import imshow\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from maskmm.models.maskrcnn import MaskRCNN\n",
    "from maskmm import learner\n",
    "from maskmm.utils import visualize\n",
    "\n",
    "from maskmm.datasets.nuke.config import Config\n",
    "from maskmm.datasets.nuke.dataset import Dataset\n",
    "\n",
    "from maskmm.mytools import *\n",
    "\n",
    "ROOT_DIR = \"/home/ubuntu/maskmm\"\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"data/models/mask_rcnn_coco.pth\")\n",
    "DATA = join(expanduser(\"~\"), \"data\", \"nuke\") \n",
    "\n",
    "class Config(Config):\n",
    "    NAME = \"mm\"\n",
    "    STEPS_PER_EPOCH = 1\n",
    "    VALIDATION_STEPS = 1\n",
    "    GPU_COUNT=0\n",
    "    NPRANDOM = True\n",
    "config = Config()\n",
    "rngreset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root:INFO]:starting (cellevents.py:36, time=12:23)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.68 ms\n"
     ]
    }
   ],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root:INFO]:starting (cellevents.py:36, time=12:23)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "train    505\n",
       "valid    123\n",
       "Name: subset, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.13 s\n"
     ]
    }
   ],
   "source": [
    "# create validation sample\n",
    "pvalid = .2\n",
    "trainpath = join(DATA, \"stage1_train\")\n",
    "df = pd.DataFrame(os.listdir(trainpath), columns=[\"image\"])\n",
    "df[\"subset\"] = np.random.random(len(df))>pvalid\n",
    "df.loc[df.subset==True, \"subset\"] = \"train\"\n",
    "df.loc[df.subset==False, \"subset\"] = \"valid\"\n",
    "\n",
    "df.to_pickle(join(DATA, \"subset.pkl\"))\n",
    "df.subset.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root:INFO]:starting (cellevents.py:36, time=12:23)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 45.7 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>audio{display:none}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_train = Dataset(config)\n",
    "dataset_train.load_nuke(trainpath, \"train\")\n",
    "dataset_train.prepare()\n",
    "\n",
    "dataset_val = Dataset(config)\n",
    "dataset_val.load_nuke(trainpath, \"valid\")\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root:INFO]:starting (cellevents.py:36, time=12:23)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.45 ms\n"
     ]
    }
   ],
   "source": [
    "%%s\n",
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root:INFO]:starting (cellevents.py:36, time=12:23)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 50.5 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>audio{display:none}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = MaskRCNN(model_dir=MODEL_DIR, config=config).to(config.DEVICE)\n",
    "model.initialize_weights()\n",
    "\n",
    "# load pretrained except final layers that depend on NUM_CLASSES\n",
    "params = torch.load(COCO_MODEL_PATH)\n",
    "params.pop('classifier.linear_class.weight')\n",
    "params.pop(\"classifier.linear_bbox.weight\")\n",
    "params.pop(\"mask.conv5.weight\")\n",
    "params.pop('classifier.linear_class.bias')\n",
    "params.pop(\"classifier.linear_bbox.bias\")\n",
    "params.pop(\"mask.conv5.bias\")\n",
    "model.load_state_dict(params, strict=False)\n",
    "_ = model.to(config.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root:INFO]:starting (cellevents.py:36, time=12:33)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 1. LR=0.01\n",
      "\n",
      "Checkpoint Path: /home/ubuntu/maskmm/logs/mm20181023_1223/mask_rcnn_mm_{:04d}.pth\n",
      "Epoch 1/3.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-0e5568cfe2e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create model in training mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"heads\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/maskmm/maskmm/learner.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, learning_rate, epochs, layers)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             losses = self.run_epoch(train_generator, model.config.STEPS_PER_EPOCH,\n\u001b[0;32m---> 90\u001b[0;31m                                     mode=\"training\")\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/maskmm/maskmm/learner.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(self, datagenerator, steps, mode)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatagenerator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0mbatch_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/src/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/maskmm/maskmm/datagen/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, image_index)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mimage_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_metas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_class_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_masks\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_image_gt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_mini_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUSE_MINI_MASK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;31m# If no instances then skip. e.g. image has none of classes we care about.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/maskmm/maskmm/datagen/dataset.py\u001b[0m in \u001b[0;36mload_image_gt\u001b[0;34m(self, image_id, use_mini_mask)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# augment image and mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;31m# Bounding boxes. some boxes might be all zeros if the corresponding mask got cropped out.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/maskmm/maskmm/utils/image_utils.py\u001b[0m in \u001b[0;36maugment\u001b[0;34m(img, masks)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugment_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mmasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugment_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 842 ms\n"
     ]
    }
   ],
   "source": [
    "# Create model in training mode\n",
    "m = learner.Learner(model, dataset_train, dataset_val)\n",
    "m.train(.01, 3, \"heads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root:INFO]:starting (cellevents.py:36, time=12:26)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.24 ms\n"
     ]
    }
   ],
   "source": [
    "torch.tensor(config.MEAN_PIXEL, dtype=torch.float).type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "match(\"images\"), match(\"gt_boxes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "match(\"anchors_pre\"), match(\"gt_boxes_pre\"), match(\"overlaps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "match(\"anchors_pre\", tol=0), match(\"gt_boxes_pre\", tol=0), match(\"overlaps\", tol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "match(\"test1\"), match(\"test2\"), match(\"test3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "match(\"rpn_match\"), match(\"rpn_bbox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "match(\"pre_conv\", tol=0), match(\"post_conv\"), match(\"post_conv\", \"post_convtest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mse(load(\"inputs1\"), load(\"inputs10\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "match(\"inputs0\"), match(\"inputs1\"), match(\"normalized_boxes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load(\"masks\").sum(),load(\"masks0\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mse(load(\"masks\"),load(\"masks0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "match(\"proposals\"), match(\"rois\"), match(\"roi_gt_class_ids\"), match(\"deltas\"), match(\"masks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%s\n",
    "from maskmm.utils import box_utils\n",
    "a=load(\"anchors_pre\")\n",
    "b=load(\"gt_boxes_pre\")\n",
    "%timeit a=load(\"anchors_pre\")\n",
    "%timeit b=load(\"gt_boxes_pre\")\n",
    "%timeit overlaps = box_utils.np_compute_overlaps(a,b)\n",
    "save(overlaps.numpy(), \"overlaps\")\n",
    "match(\"overlaps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%s\n",
    "from maskmm.utils import box_utils\n",
    "a=torch.Tensor(load(\"anchors_pre\")).cpu()\n",
    "b=torch.Tensor(load(\"gt_boxes_pre\")).cpu()\n",
    "%timeit a=torch.Tensor(load(\"anchors_pre\")).cpu()\n",
    "%timeit b=torch.Tensor(load(\"gt_boxes_pre\")).cpu()\n",
    "%timeit overlaps = box_utils.torch_compute_overlaps(a,b)\n",
    "save(overlaps.numpy(), \"overlaps\")\n",
    "match(\"overlaps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%s\n",
    "from maskmm.utils import box_utils\n",
    "a=torch.Tensor(load(\"anchors_pre\")).cuda()\n",
    "b=torch.Tensor(load(\"gt_boxes_pre\")).cuda()\n",
    "%timeit a=torch.Tensor(load(\"anchors_pre\")).cuda()\n",
    "%timeit b=torch.Tensor(load(\"gt_boxes_pre\")).cuda()\n",
    "%timeit overlaps = box_utils.torch_compute_overlaps(a,b)\n",
    "save(overlaps.numpy(), \"overlaps\")\n",
    "match(\"overlaps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
