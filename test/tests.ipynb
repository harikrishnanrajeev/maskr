{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root:INFO]: (ipstartup.py:17, time=19:04)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>audio{display:none}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from maskmm.ipstartup import *\n",
    "import torch\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "from maskmm.config import Config\n",
    "import pytest\n",
    "from maskmm.baseline import Test, Baseline, match\n",
    "import model\n",
    "base = Baseline(\"maskmm0\")\n",
    "t = Test(base, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root:INFO]:starting (cellevents.py:36, time=19:05)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.6.6, pytest-3.10.1, py-1.5.3, pluggy-0.7.1\n",
      "rootdir: /home/ubuntu/maskmm/test, inifile: pytest.ini\n",
      "collected 37 items\n",
      "\n",
      "../../test/test_datagen.py ........                                      [ 21%]\n",
      "../../test/test_filters.py .........                                     [ 45%]\n",
      "../../test/test_loss.py FFFF....FFFFFFFFFFFF                             [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "______________________________ test_rpn_class[0] _______________________________\n",
      "\n",
      "t = <maskmm.baseline.Test object at 0x7f2ba8205438>, index = 0\n",
      "\n",
      "    @pytest.mark.parametrize(\"index\", range(4))\n",
      "    def test_rpn_class(t, index):\n",
      ">       t.run(model.compute_rpn_class_loss, loss.rpn_class)\n",
      "\n",
      "../../test/test_loss.py:9: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "../../maskmm/baseline.py:196: in run\n",
      "    self.results = self.func(*self.inputs)\n",
      "../../maskmm/loss.py:29: in rpn_class\n",
      "    loss = F.cross_entropy(rpn_class_logits, anchor_class)\n",
      "../../../src/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:1646: in cross_entropy\n",
      "    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "input = tensor([[  0.0000, -16.4533],\n",
      "        [  0.0000, -17.6597],\n",
      "        [ -0.0000, -14.6390],\n",
      "        [  0.0000, -15.3835]...       [ -0.0002,  -8.6816],\n",
      "        [ -0.0001,  -9.4584],\n",
      "        [ -0.0019,  -6.2585]], grad_fn=<LogSoftmaxBackward>)\n",
      "target = tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      " ...,\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]])\n",
      "weight = None, size_average = None, ignore_index = -100, reduce = None\n",
      "reduction = 'elementwise_mean'\n",
      "\n",
      "    def nll_loss(input, target, weight=None, size_average=None, ignore_index=-100,\n",
      "                 reduce=None, reduction='elementwise_mean'):\n",
      "        r\"\"\"The negative log likelihood loss.\n",
      "    \n",
      "        See :class:`~torch.nn.NLLLoss` for details.\n",
      "    \n",
      "        Args:\n",
      "            input: :math:`(N, C)` where `C = number of classes` or :math:`(N, C, H, W)`\n",
      "                in case of 2D Loss, or :math:`(N, C, d_1, d_2, ..., d_K)` where :math:`K > 1`\n",
      "                in the case of K-dimensional loss.\n",
      "            target: :math:`(N)` where each value is :math:`0 \\leq \\text{targets}[i] \\leq C-1`,\n",
      "                or :math:`(N, d_1, d_2, ..., d_K)` where :math:`K \\geq 1` for\n",
      "                K-dimensional loss.\n",
      "            weight (Tensor, optional): a manual rescaling weight given to each\n",
      "                class. If given, has to be a Tensor of size `C`\n",
      "            size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
      "                the losses are averaged over each loss element in the batch. Note that for\n",
      "                some losses, there multiple elements per sample. If the field :attr:`size_average`\n",
      "                is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
      "                when reduce is ``False``. Default: ``True``\n",
      "            ignore_index (int, optional): Specifies a target value that is ignored\n",
      "                and does not contribute to the input gradient. When :attr:`size_average` is\n",
      "                ``True``, the loss is averaged over non-ignored targets. Default: -100\n",
      "            reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
      "                losses are averaged or summed over observations for each minibatch depending\n",
      "                on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
      "                batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
      "            reduction (string, optional): Specifies the reduction to apply to the output:\n",
      "                'none' | 'elementwise_mean' | 'sum'. 'none': no reduction will be applied,\n",
      "                'elementwise_mean': the sum of the output will be divided by the number of\n",
      "                elements in the output, 'sum': the output will be summed. Note: :attr:`size_average`\n",
      "                and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n",
      "                specifying either of those two args will override :attr:`reduction`. Default: 'elementwise_mean'\n",
      "    \n",
      "        Example::\n",
      "    \n",
      "            >>> # input is of size N x C = 3 x 5\n",
      "            >>> input = torch.randn(3, 5, requires_grad=True)\n",
      "            >>> # each element in target has to have 0 <= value < C\n",
      "            >>> target = torch.tensor([1, 0, 4])\n",
      "            >>> output = F.nll_loss(F.log_softmax(input), target)\n",
      "            >>> output.backward()\n",
      "        \"\"\"\n",
      "        if size_average is not None or reduce is not None:\n",
      "            reduction = _Reduction.legacy_get_string(size_average, reduce)\n",
      "        dim = input.dim()\n",
      "        if dim < 2:\n",
      "            raise ValueError('Expected 2 or more dimensions (got {})'.format(dim))\n",
      "    \n",
      "        if input.size(0) != target.size(0):\n",
      "            raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n",
      "                             .format(input.size(0), target.size(0)))\n",
      "        if dim == 2:\n",
      ">           return torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\n",
      "E           RuntimeError: multi-target not supported at /opt/conda/conda-bld/pytorch-nightly_1538165619353/work/aten/src/THCUNN/generic/ClassNLLCriterion.cu:15\n",
      "\n",
      "../../../src/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:1503: RuntimeError\n",
      "______________________________ test_rpn_class[1] _______________________________\n",
      "\n",
      "t = <maskmm.baseline.Test object at 0x7f2ba806d0b8>, index = 1\n",
      "\n",
      "    @pytest.mark.parametrize(\"index\", range(4))\n",
      "    def test_rpn_class(t, index):\n",
      ">       t.run(model.compute_rpn_class_loss, loss.rpn_class)\n",
      "\n",
      "../../test/test_loss.py:9: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "../../maskmm/baseline.py:196: in run\n",
      "    self.results = self.func(*self.inputs)\n",
      "../../maskmm/loss.py:29: in rpn_class\n",
      "    loss = F.cross_entropy(rpn_class_logits, anchor_class)\n",
      "../../../src/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:1646: in cross_entropy\n",
      "    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "input = tensor([[  0.0000, -16.4533],\n",
      "        [  0.0000, -17.6597],\n",
      "        [ -0.0000, -14.6390],\n",
      "        [  0.0000, -15.3835]...       [ -0.0002,  -8.6816],\n",
      "        [ -0.0001,  -9.4584],\n",
      "        [ -0.0019,  -6.2585]], grad_fn=<LogSoftmaxBackward>)\n",
      "target = tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      " ...,\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]])\n",
      "weight = None, size_average = None, ignore_index = -100, reduce = None\n",
      "reduction = 'elementwise_mean'\n",
      "\n",
      "    def nll_loss(input, target, weight=None, size_average=None, ignore_index=-100,\n",
      "                 reduce=None, reduction='elementwise_mean'):\n",
      "        r\"\"\"The negative log likelihood loss.\n",
      "    \n",
      "        See :class:`~torch.nn.NLLLoss` for details.\n",
      "    \n",
      "        Args:\n",
      "            input: :math:`(N, C)` where `C = number of classes` or :math:`(N, C, H, W)`\n",
      "                in case of 2D Loss, or :math:`(N, C, d_1, d_2, ..., d_K)` where :math:`K > 1`\n",
      "                in the case of K-dimensional loss.\n",
      "            target: :math:`(N)` where each value is :math:`0 \\leq \\text{targets}[i] \\leq C-1`,\n",
      "                or :math:`(N, d_1, d_2, ..., d_K)` where :math:`K \\geq 1` for\n",
      "                K-dimensional loss.\n",
      "            weight (Tensor, optional): a manual rescaling weight given to each\n",
      "                class. If given, has to be a Tensor of size `C`\n",
      "            size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                the losses are averaged over each loss element in the batch. Note that for\n",
      "                some losses, there multiple elements per sample. If the field :attr:`size_average`\n",
      "                is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
      "                when reduce is ``False``. Default: ``True``\n",
      "            ignore_index (int, optional): Specifies a target value that is ignored\n",
      "                and does not contribute to the input gradient. When :attr:`size_average` is\n",
      "                ``True``, the loss is averaged over non-ignored targets. Default: -100\n",
      "            reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
      "                losses are averaged or summed over observations for each minibatch depending\n",
      "                on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
      "                batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
      "            reduction (string, optional): Specifies the reduction to apply to the output:\n",
      "                'none' | 'elementwise_mean' | 'sum'. 'none': no reduction will be applied,\n",
      "                'elementwise_mean': the sum of the output will be divided by the number of\n",
      "                elements in the output, 'sum': the output will be summed. Note: :attr:`size_average`\n",
      "                and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n",
      "                specifying either of those two args will override :attr:`reduction`. Default: 'elementwise_mean'\n",
      "    \n",
      "        Example::\n",
      "    \n",
      "            >>> # input is of size N x C = 3 x 5\n",
      "            >>> input = torch.randn(3, 5, requires_grad=True)\n",
      "            >>> # each element in target has to have 0 <= value < C\n",
      "            >>> target = torch.tensor([1, 0, 4])\n",
      "            >>> output = F.nll_loss(F.log_softmax(input), target)\n",
      "            >>> output.backward()\n",
      "        \"\"\"\n",
      "        if size_average is not None or reduce is not None:\n",
      "            reduction = _Reduction.legacy_get_string(size_average, reduce)\n",
      "        dim = input.dim()\n",
      "        if dim < 2:\n",
      "            raise ValueError('Expected 2 or more dimensions (got {})'.format(dim))\n",
      "    \n",
      "        if input.size(0) != target.size(0):\n",
      "            raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n",
      "                             .format(input.size(0), target.size(0)))\n",
      "        if dim == 2:\n",
      ">           return torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\n",
      "E           RuntimeError: multi-target not supported at /opt/conda/conda-bld/pytorch-nightly_1538165619353/work/aten/src/THCUNN/generic/ClassNLLCriterion.cu:15\n",
      "\n",
      "../../../src/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:1503: RuntimeError\n",
      "______________________________ test_rpn_class[2] _______________________________\n",
      "\n",
      "t = <maskmm.baseline.Test object at 0x7f2ba81edc18>, index = 2\n",
      "\n",
      "    @pytest.mark.parametrize(\"index\", range(4))\n",
      "    def test_rpn_class(t, index):\n",
      ">       t.run(model.compute_rpn_class_loss, loss.rpn_class)\n",
      "\n",
      "../../test/test_loss.py:9: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "../../maskmm/baseline.py:196: in run\n",
      "    self.results = self.func(*self.inputs)\n",
      "../../maskmm/loss.py:29: in rpn_class\n",
      "    loss = F.cross_entropy(rpn_class_logits, anchor_class)\n",
      "../../../src/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:1646: in cross_entropy\n",
      "    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "input = tensor([[  0.0000, -16.4533],\n",
      "        [  0.0000, -17.6597],\n",
      "        [ -0.0000, -14.6390],\n",
      "        [  0.0000, -15.3835]...       [ -0.0002,  -8.6816],\n",
      "        [ -0.0001,  -9.4584],\n",
      "        [ -0.0019,  -6.2585]], grad_fn=<LogSoftmaxBackward>)\n",
      "target = tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      " ...,\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]])\n",
      "weight = None, size_average = None, ignore_index = -100, reduce = None\n",
      "reduction = 'elementwise_mean'\n",
      "\n",
      "    def nll_loss(input, target, weight=None, size_average=None, ignore_index=-100,\n",
      "                 reduce=None, reduction='elementwise_mean'):\n",
      "        r\"\"\"The negative log likelihood loss.\n",
      "    \n",
      "        See :class:`~torch.nn.NLLLoss` for details.\n",
      "    \n",
      "        Args:\n",
      "            input: :math:`(N, C)` where `C = number of classes` or :math:`(N, C, H, W)`\n",
      "                in case of 2D Loss, or :math:`(N, C, d_1, d_2, ..., d_K)` where :math:`K > 1`\n",
      "                in the case of K-dimensional loss.\n",
      "            target: :math:`(N)` where each value is :math:`0 \\leq \\text{targets}[i] \\leq C-1`,\n",
      "                or :math:`(N, d_1, d_2, ..., d_K)` where :math:`K \\geq 1` for\n",
      "                K-dimensional loss.\n",
      "            weight (Tensor, optional): a manual rescaling weight given to each\n",
      "                class. If given, has to be a Tensor of size `C`\n",
      "            size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
      "                the losses are averaged over each loss element in the batch. Note that for\n",
      "                some losses, there multiple elements per sample. If the field :attr:`size_average`\n",
      "                is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
      "                when reduce is ``False``. Default: ``True``\n",
      "            ignore_index (int, optional): Specifies a target value that is ignored\n",
      "                and does not contribute to the input gradient. When :attr:`size_average` is\n",
      "                ``True``, the loss is averaged over non-ignored targets. Default: -100\n",
      "            reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
      "                losses are averaged or summed over observations for each minibatch depending\n",
      "                on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
      "                batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
      "            reduction (string, optional): Specifies the reduction to apply to the output:\n",
      "                'none' | 'elementwise_mean' | 'sum'. 'none': no reduction will be applied,\n",
      "                'elementwise_mean': the sum of the output will be divided by the number of\n",
      "                elements in the output, 'sum': the output will be summed. Note: :attr:`size_average`\n",
      "                and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n",
      "                specifying either of those two args will override :attr:`reduction`. Default: 'elementwise_mean'\n",
      "    \n",
      "        Example::\n",
      "    \n",
      "            >>> # input is of size N x C = 3 x 5\n",
      "            >>> input = torch.randn(3, 5, requires_grad=True)\n",
      "            >>> # each element in target has to have 0 <= value < C\n",
      "            >>> target = torch.tensor([1, 0, 4])\n",
      "            >>> output = F.nll_loss(F.log_softmax(input), target)\n",
      "            >>> output.backward()\n",
      "        \"\"\"\n",
      "        if size_average is not None or reduce is not None:\n",
      "            reduction = _Reduction.legacy_get_string(size_average, reduce)\n",
      "        dim = input.dim()\n",
      "        if dim < 2:\n",
      "            raise ValueError('Expected 2 or more dimensions (got {})'.format(dim))\n",
      "    \n",
      "        if input.size(0) != target.size(0):\n",
      "            raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n",
      "                             .format(input.size(0), target.size(0)))\n",
      "        if dim == 2:\n",
      ">           return torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\n",
      "E           RuntimeError: multi-target not supported at /opt/conda/conda-bld/pytorch-nightly_1538165619353/work/aten/src/THCUNN/generic/ClassNLLCriterion.cu:15\n",
      "\n",
      "../../../src/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:1503: RuntimeError\n",
      "______________________________ test_rpn_class[3] _______________________________\n",
      "\n",
      "t = <maskmm.baseline.Test object at 0x7f2ba81af748>, index = 3\n",
      "\n",
      "    @pytest.mark.parametrize(\"index\", range(4))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def test_rpn_class(t, index):\n",
      ">       t.run(model.compute_rpn_class_loss, loss.rpn_class)\n",
      "\n",
      "../../test/test_loss.py:9: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "../../maskmm/baseline.py:196: in run\n",
      "    self.results = self.func(*self.inputs)\n",
      "../../maskmm/loss.py:29: in rpn_class\n",
      "    loss = F.cross_entropy(rpn_class_logits, anchor_class)\n",
      "../../../src/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:1646: in cross_entropy\n",
      "    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "input = tensor([[  0.0000, -16.4533],\n",
      "        [  0.0000, -17.6597],\n",
      "        [ -0.0000, -14.6390],\n",
      "        [  0.0000, -15.3835]...       [ -0.0002,  -8.6816],\n",
      "        [ -0.0001,  -9.4584],\n",
      "        [ -0.0019,  -6.2585]], grad_fn=<LogSoftmaxBackward>)\n",
      "target = tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      " ...,\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]])\n",
      "weight = None, size_average = None, ignore_index = -100, reduce = None\n",
      "reduction = 'elementwise_mean'\n",
      "\n",
      "    def nll_loss(input, target, weight=None, size_average=None, ignore_index=-100,\n",
      "                 reduce=None, reduction='elementwise_mean'):\n",
      "        r\"\"\"The negative log likelihood loss.\n",
      "    \n",
      "        See :class:`~torch.nn.NLLLoss` for details.\n",
      "    \n",
      "        Args:\n",
      "            input: :math:`(N, C)` where `C = number of classes` or :math:`(N, C, H, W)`\n",
      "                in case of 2D Loss, or :math:`(N, C, d_1, d_2, ..., d_K)` where :math:`K > 1`\n",
      "                in the case of K-dimensional loss.\n",
      "            target: :math:`(N)` where each value is :math:`0 \\leq \\text{targets}[i] \\leq C-1`,\n",
      "                or :math:`(N, d_1, d_2, ..., d_K)` where :math:`K \\geq 1` for\n",
      "                K-dimensional loss.\n",
      "            weight (Tensor, optional): a manual rescaling weight given to each\n",
      "                class. If given, has to be a Tensor of size `C`\n",
      "            size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
      "                the losses are averaged over each loss element in the batch. Note that for\n",
      "                some losses, there multiple elements per sample. If the field :attr:`size_average`\n",
      "                is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
      "                when reduce is ``False``. Default: ``True``\n",
      "            ignore_index (int, optional): Specifies a target value that is ignored\n",
      "                and does not contribute to the input gradient. When :attr:`size_average` is\n",
      "                ``True``, the loss is averaged over non-ignored targets. Default: -100\n",
      "            reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
      "                losses are averaged or summed over observations for each minibatch depending\n",
      "                on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
      "                batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
      "            reduction (string, optional): Specifies the reduction to apply to the output:\n",
      "                'none' | 'elementwise_mean' | 'sum'. 'none': no reduction will be applied,\n",
      "                'elementwise_mean': the sum of the output will be divided by the number of\n",
      "                elements in the output, 'sum': the output will be summed. Note: :attr:`size_average`\n",
      "                and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n",
      "                specifying either of those two args will override :attr:`reduction`. Default: 'elementwise_mean'\n",
      "    \n",
      "        Example::\n",
      "    \n",
      "            >>> # input is of size N x C = 3 x 5\n",
      "            >>> input = torch.randn(3, 5, requires_grad=True)\n",
      "            >>> # each element in target has to have 0 <= value < C\n",
      "            >>> target = torch.tensor([1, 0, 4])\n",
      "            >>> output = F.nll_loss(F.log_softmax(input), target)\n",
      "            >>> output.backward()\n",
      "        \"\"\"\n",
      "        if size_average is not None or reduce is not None:\n",
      "            reduction = _Reduction.legacy_get_string(size_average, reduce)\n",
      "        dim = input.dim()\n",
      "        if dim < 2:\n",
      "            raise ValueError('Expected 2 or more dimensions (got {})'.format(dim))\n",
      "    \n",
      "        if input.size(0) != target.size(0):\n",
      "            raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n",
      "                             .format(input.size(0), target.size(0)))\n",
      "        if dim == 2:\n",
      ">           return torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\n",
      "E           RuntimeError: multi-target not supported at /opt/conda/conda-bld/pytorch-nightly_1538165619353/work/aten/src/THCUNN/generic/ClassNLLCriterion.cu:15\n",
      "\n",
      "../../../src/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:1503: RuntimeError\n",
      "_____________________________ test_mrcnn_class[0] ______________________________\n",
      "\n",
      "t = <maskmm.baseline.Test object at 0x7f2ba806d4a8>, index = 0\n",
      "\n",
      "    @pytest.mark.parametrize(\"index\", range(4))\n",
      "    def test_mrcnn_class(t, index):\n",
      ">       t.run(model.compute_mrcnn_class_loss, loss.mrcnn_class)\n",
      "\n",
      "../../test/test_loss.py:17: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "../../maskmm/baseline.py:196: in run\n",
      "    self.results = self.func(*self.inputs)\n",
      "../../maskmm/loss.py:83: in mrcnn_class\n",
      "    loss = F.cross_entropy(pred_class_logits, target_class_ids.long())\n",
      "../../../src/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:1646: in cross_entropy\n",
      "    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "input = tensor([ 0.2562, -0.2182,  0.1678, -0.1934, -0.1705, -0.1403,  0.2803, -0.0722,\n",
      "        -0.6480, -0.5077, -0.1638, -0....  0.0133, -0.1864,  0.0405,  0.1589,  0.3911,  0.0419,  0.1259,\n",
      "         0.1187, -0.0246], grad_fn=<AsStridedBackward>)\n",
      "dim = 1, _stacklevel = 3\n",
      "\n",
      "    def log_softmax(input, dim=None, _stacklevel=3):\n",
      "        r\"\"\"Applies a softmax followed by a logarithm.\n",
      "    \n",
      "        While mathematically equivalent to log(softmax(x)), doing these two\n",
      "        operations separately is slower, and numerically unstable. This function\n",
      "        uses an alternative formulation to compute the output and gradient correctly.\n",
      "    \n",
      "        See :class:`~torch.nn.LogSoftmax` for more details.\n",
      "    \n",
      "        Arguments:\n",
      "            input (Tensor): input\n",
      "            dim (int): A dimension along which log_softmax will be computed.\n",
      "        \"\"\"\n",
      "        if dim is None:\n",
      "            dim = _get_softmax_dim('log_softmax', input.dim(), _stacklevel)\n",
      ">       return input.log_softmax(dim)\n",
      "E       RuntimeError: Dimension out of range (expected to be in range of [-1, 0], but got 1)\n",
      "\n",
      "../../../src/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:1070: RuntimeError\n",
      "_____________________________ test_mrcnn_class[1] ______________________________\n",
      "\n",
      "t = <maskmm.baseline.Test object at 0x7f2ba80f4080>, index = 1\n",
      "\n",
      "    @pytest.mark.parametrize(\"index\", range(4))\n",
      "    def test_mrcnn_class(t, index):\n",
      ">       t.run(model.compute_mrcnn_class_loss, loss.mrcnn_class)\n",
      "\n",
      "../../test/test_loss.py:17: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "../../maskmm/baseline.py:196: in run\n",
      "    self.results = self.func(*self.inputs)\n",
      "../../maskmm/loss.py:83: in mrcnn_class\n",
      "    loss = F.cross_entropy(pred_class_logits, target_class_ids.long())\n",
      "../../../src/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:1646: in cross_entropy\n",
      "    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "input = tensor([ 0.2562, -0.2182,  0.1678, -0.1934, -0.1705, -0.1403,  0.2803, -0.0722,\n",
      "        -0.6480, -0.5077, -0.1638, -0....  0.0133, -0.1864,  0.0405,  0.1589,  0.3911,  0.0419,  0.1259,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0.1187, -0.0246], grad_fn=<AsStridedBackward>)\n",
      "dim = 1, _stacklevel = 3\n",
      "\n",
      "    def log_softmax(input, dim=None, _stacklevel=3):\n",
      "        r\"\"\"Applies a softmax followed by a logarithm.\n",
      "    \n",
      "        While mathematically equivalent to log(softmax(x)), doing these two\n",
      "        operations separately is slower, and numerically unstable. This function\n",
      "        uses an alternative formulation to compute the output and gradient correctly.\n",
      "    \n",
      "        See :class:`~torch.nn.LogSoftmax` for more details.\n",
      "    \n",
      "        Arguments:\n",
      "            input (Tensor): input\n",
      "            dim (int): A dimension along which log_softmax will be computed.\n",
      "        \"\"\"\n",
      "        if dim is None:\n",
      "            dim = _get_softmax_dim('log_softmax', input.dim(), _stacklevel)\n",
      ">       return input.log_softmax(dim)\n",
      "E       RuntimeError: Dimension out of range (expected to be in range of [-1, 0], but got 1)\n",
      "\n",
      "../../../src/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:1070: RuntimeError\n",
      "_____________________________ test_mrcnn_class[2] ______________________________\n",
      "\n",
      "t = <maskmm.baseline.Test object at 0x7f2ba80c8470>, index = 2\n",
      "\n",
      "    @pytest.mark.parametrize(\"index\", range(4))\n",
      "    def test_mrcnn_class(t, index):\n",
      ">       t.run(model.compute_mrcnn_class_loss, loss.mrcnn_class)\n",
      "\n",
      "../../test/test_loss.py:17: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "../../maskmm/baseline.py:196: in run\n",
      "    self.results = self.func(*self.inputs)\n",
      "../../maskmm/loss.py:83: in mrcnn_class\n",
      "    loss = F.cross_entropy(pred_class_logits, target_class_ids.long())\n",
      "../../../src/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:1646: in cross_entropy\n",
      "    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "input = tensor([ 0.2562, -0.2182,  0.1678, -0.1934, -0.1705, -0.1403,  0.2803, -0.0722,\n",
      "        -0.6480, -0.5077, -0.1638, -0....  0.0133, -0.1864,  0.0405,  0.1589,  0.3911,  0.0419,  0.1259,\n",
      "         0.1187, -0.0246], grad_fn=<AsStridedBackward>)\n",
      "dim = 1, _stacklevel = 3\n",
      "\n",
      "    def log_softmax(input, dim=None, _stacklevel=3):\n",
      "        r\"\"\"Applies a softmax followed by a logarithm.\n",
      "    \n",
      "        While mathematically equivalent to log(softmax(x)), doing these two\n",
      "        operations separately is slower, and numerically unstable. This function\n",
      "        uses an alternative formulation to compute the output and gradient correctly.\n",
      "    \n",
      "        See :class:`~torch.nn.LogSoftmax` for more details.\n",
      "    \n",
      "        Arguments:\n",
      "            input (Tensor): input\n",
      "            dim (int): A dimension along which log_softmax will be computed.\n",
      "        \"\"\"\n",
      "        if dim is None:\n",
      "            dim = _get_softmax_dim('log_softmax', input.dim(), _stacklevel)\n",
      ">       return input.log_softmax(dim)\n",
      "E       RuntimeError: Dimension out of range (expected to be in range of [-1, 0], but got 1)\n",
      "\n",
      "../../../src/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:1070: RuntimeError\n",
      "_____________________________ test_mrcnn_class[3] ______________________________\n",
      "\n",
      "t = <maskmm.baseline.Test object at 0x7f2ba8033278>, index = 3\n",
      "\n",
      "    @pytest.mark.parametrize(\"index\", range(4))\n",
      "    def test_mrcnn_class(t, index):\n",
      ">       t.run(model.compute_mrcnn_class_loss, loss.mrcnn_class)\n",
      "\n",
      "../../test/test_loss.py:17: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "../../maskmm/baseline.py:196: in run\n",
      "    self.results = self.func(*self.inputs)\n",
      "../../maskmm/loss.py:83: in mrcnn_class\n",
      "    loss = F.cross_entropy(pred_class_logits, target_class_ids.long())\n",
      "../../../src/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:1646: in cross_entropy\n",
      "    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "input = tensor([ 0.2562, -0.2182,  0.1678, -0.1934, -0.1705, -0.1403,  0.2803, -0.0722,\n",
      "        -0.6480, -0.5077, -0.1638, -0....  0.0133, -0.1864,  0.0405,  0.1589,  0.3911,  0.0419,  0.1259,\n",
      "         0.1187, -0.0246], grad_fn=<AsStridedBackward>)\n",
      "dim = 1, _stacklevel = 3\n",
      "\n",
      "    def log_softmax(input, dim=None, _stacklevel=3):\n",
      "        r\"\"\"Applies a softmax followed by a logarithm.\n",
      "    \n",
      "        While mathematically equivalent to log(softmax(x)), doing these two\n",
      "        operations separately is slower, and numerically unstable. This function\n",
      "        uses an alternative formulation to compute the output and gradient correctly.\n",
      "    \n",
      "        See :class:`~torch.nn.LogSoftmax` for more details.\n",
      "    \n",
      "        Arguments:\n",
      "            input (Tensor): input\n",
      "            dim (int): A dimension along which log_softmax will be computed.\n",
      "        \"\"\"\n",
      "        if dim is None:\n",
      "            dim = _get_softmax_dim('log_softmax', input.dim(), _stacklevel)\n",
      ">       return input.log_softmax(dim)\n",
      "E       RuntimeError: Dimension out of range (expected to be in range of [-1, 0], but got 1)\n",
      "\n",
      "../../../src/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:1070: RuntimeError\n",
      "______________________________ test_mrcnn_bbox[0] ______________________________\n",
      "\n",
      "t = <maskmm.baseline.Test object at 0x7f2ba80a5400>, index = 0\n",
      "\n",
      "    @pytest.mark.parametrize(\"index\", range(4))\n",
      "    def test_mrcnn_bbox(t, index):\n",
      ">       t.run(model.compute_mrcnn_bbox_loss, loss.mrcnn_bbox)\n",
      "\n",
      "../../test/test_loss.py:21: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "../../maskmm/baseline.py:196: in run\n",
      "    self.results = self.func(*self.inputs)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "target_bbox = tensor([-0.8207, -0.4255,  1.2909,  1.1883,  0.5332, -2.3162, -0.4287,  0.8678,\n",
      "        -0.0631,  0.6729, -0.2987, -0....    0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000])\n",
      "target_class_ids = tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "pred_bbox = tensor([[ 0.3104, -0.0039,  0.5234,  0.2698],\n",
      "        [-0.6332, -0.5304,  0.0569, -0.1449],\n",
      "        [ 0.0336,  0.1035,...      [ 0.1433, -0.7981,  0.4616, -0.3784],\n",
      "        [-0.0305, -0.1762,  0.0261,  0.2933]], grad_fn=<AsStridedBackward>)\n",
      "\n",
      "    @saveall\n",
      "    def mrcnn_bbox(target_bbox, target_class_ids, pred_bbox):\n",
      "        \"\"\"Loss for Mask R-CNN bounding box refinement.\n",
      "    \n",
      "        target_bbox: [batch, num_rois, (dy, dx, log(dh), log(dw))]\n",
      "        target_class_ids: [batch, num_rois]. Integer class IDs.\n",
      "        pred_bbox: [batch, num_rois, num_classes, (dy, dx, log(dh), log(dw))]\n",
      "        \"\"\"\n",
      "        # todo review what is zeropadded and where\n",
      "    \n",
      "        # todo bs>1. need to do all the below for each batch OR vectorize\n",
      "        target_bbox, target_class_ids, pred_bbox = unbatch(target_bbox, target_class_ids, pred_bbox)\n",
      "    \n",
      "        if len(target_class_ids):\n",
      "            # Only positive ROIs contribute to the loss. And only\n",
      "            # the right class_id of each ROI. Get their indicies.\n",
      "            positive_roi_ix = torch.nonzero(target_class_ids > 0)[:, 0]\n",
      "            positive_roi_class_ids = target_class_ids[positive_roi_ix].long()\n",
      "            indices = torch.stack((positive_roi_ix, positive_roi_class_ids), dim=1)\n",
      "    \n",
      "            # Gather the deltas (predicted and true) that contribute to loss\n",
      ">           target_bbox = target_bbox[indices[:, 0], :]\n",
      "E           IndexError: too many indices for tensor of dimension 1\n",
      "\n",
      "../../maskmm/loss.py:110: IndexError\n",
      "______________________________ test_mrcnn_bbox[1] ______________________________\n",
      "\n",
      "t = <maskmm.baseline.Test object at 0x7f2ba80f7b38>, index = 1\n",
      "\n",
      "    @pytest.mark.parametrize(\"index\", range(4))\n",
      "    def test_mrcnn_bbox(t, index):\n",
      ">       t.run(model.compute_mrcnn_bbox_loss, loss.mrcnn_bbox)\n",
      "\n",
      "../../test/test_loss.py:21: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "../../maskmm/baseline.py:196: in run\n",
      "    self.results = self.func(*self.inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "target_bbox = tensor([-0.8207, -0.4255,  1.2909,  1.1883,  0.5332, -2.3162, -0.4287,  0.8678,\n",
      "        -0.0631,  0.6729, -0.2987, -0....    0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000])\n",
      "target_class_ids = tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "pred_bbox = tensor([[ 0.3104, -0.0039,  0.5234,  0.2698],\n",
      "        [-0.6332, -0.5304,  0.0569, -0.1449],\n",
      "        [ 0.0336,  0.1035,...      [ 0.1433, -0.7981,  0.4616, -0.3784],\n",
      "        [-0.0305, -0.1762,  0.0261,  0.2933]], grad_fn=<AsStridedBackward>)\n",
      "\n",
      "    @saveall\n",
      "    def mrcnn_bbox(target_bbox, target_class_ids, pred_bbox):\n",
      "        \"\"\"Loss for Mask R-CNN bounding box refinement.\n",
      "    \n",
      "        target_bbox: [batch, num_rois, (dy, dx, log(dh), log(dw))]\n",
      "        target_class_ids: [batch, num_rois]. Integer class IDs.\n",
      "        pred_bbox: [batch, num_rois, num_classes, (dy, dx, log(dh), log(dw))]\n",
      "        \"\"\"\n",
      "        # todo review what is zeropadded and where\n",
      "    \n",
      "        # todo bs>1. need to do all the below for each batch OR vectorize\n",
      "        target_bbox, target_class_ids, pred_bbox = unbatch(target_bbox, target_class_ids, pred_bbox)\n",
      "    \n",
      "        if len(target_class_ids):\n",
      "            # Only positive ROIs contribute to the loss. And only\n",
      "            # the right class_id of each ROI. Get their indicies.\n",
      "            positive_roi_ix = torch.nonzero(target_class_ids > 0)[:, 0]\n",
      "            positive_roi_class_ids = target_class_ids[positive_roi_ix].long()\n",
      "            indices = torch.stack((positive_roi_ix, positive_roi_class_ids), dim=1)\n",
      "    \n",
      "            # Gather the deltas (predicted and true) that contribute to loss\n",
      ">           target_bbox = target_bbox[indices[:, 0], :]\n",
      "E           IndexError: too many indices for tensor of dimension 1\n",
      "\n",
      "../../maskmm/loss.py:110: IndexError\n",
      "______________________________ test_mrcnn_bbox[2] ______________________________\n",
      "\n",
      "t = <maskmm.baseline.Test object at 0x7f2ba805e3c8>, index = 2\n",
      "\n",
      "    @pytest.mark.parametrize(\"index\", range(4))\n",
      "    def test_mrcnn_bbox(t, index):\n",
      ">       t.run(model.compute_mrcnn_bbox_loss, loss.mrcnn_bbox)\n",
      "\n",
      "../../test/test_loss.py:21: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "../../maskmm/baseline.py:196: in run\n",
      "    self.results = self.func(*self.inputs)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "target_bbox = tensor([-0.8207, -0.4255,  1.2909,  1.1883,  0.5332, -2.3162, -0.4287,  0.8678,\n",
      "        -0.0631,  0.6729, -0.2987, -0....    0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000])\n",
      "target_class_ids = tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "pred_bbox = tensor([[ 0.3104, -0.0039,  0.5234,  0.2698],\n",
      "        [-0.6332, -0.5304,  0.0569, -0.1449],\n",
      "        [ 0.0336,  0.1035,...      [ 0.1433, -0.7981,  0.4616, -0.3784],\n",
      "        [-0.0305, -0.1762,  0.0261,  0.2933]], grad_fn=<AsStridedBackward>)\n",
      "\n",
      "    @saveall\n",
      "    def mrcnn_bbox(target_bbox, target_class_ids, pred_bbox):\n",
      "        \"\"\"Loss for Mask R-CNN bounding box refinement.\n",
      "    \n",
      "        target_bbox: [batch, num_rois, (dy, dx, log(dh), log(dw))]\n",
      "        target_class_ids: [batch, num_rois]. Integer class IDs.\n",
      "        pred_bbox: [batch, num_rois, num_classes, (dy, dx, log(dh), log(dw))]\n",
      "        \"\"\"\n",
      "        # todo review what is zeropadded and where\n",
      "    \n",
      "        # todo bs>1. need to do all the below for each batch OR vectorize\n",
      "        target_bbox, target_class_ids, pred_bbox = unbatch(target_bbox, target_class_ids, pred_bbox)\n",
      "    \n",
      "        if len(target_class_ids):\n",
      "            # Only positive ROIs contribute to the loss. And only\n",
      "            # the right class_id of each ROI. Get their indicies.\n",
      "            positive_roi_ix = torch.nonzero(target_class_ids > 0)[:, 0]\n",
      "            positive_roi_class_ids = target_class_ids[positive_roi_ix].long()\n",
      "            indices = torch.stack((positive_roi_ix, positive_roi_class_ids), dim=1)\n",
      "    \n",
      "            # Gather the deltas (predicted and true) that contribute to loss\n",
      ">           target_bbox = target_bbox[indices[:, 0], :]\n",
      "E           IndexError: too many indices for tensor of dimension 1\n",
      "\n",
      "../../maskmm/loss.py:110: IndexError\n",
      "______________________________ test_mrcnn_bbox[3] ______________________________\n",
      "\n",
      "t = <maskmm.baseline.Test object at 0x7f2ba80a5780>, index = 3\n",
      "\n",
      "    @pytest.mark.parametrize(\"index\", range(4))\n",
      "    def test_mrcnn_bbox(t, index):\n",
      ">       t.run(model.compute_mrcnn_bbox_loss, loss.mrcnn_bbox)\n",
      "\n",
      "../../test/test_loss.py:21: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "../../maskmm/baseline.py:196: in run\n",
      "    self.results = self.func(*self.inputs)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "target_bbox = tensor([-0.8207, -0.4255,  1.2909,  1.1883,  0.5332, -2.3162, -0.4287,  0.8678,\n",
      "        -0.0631,  0.6729, -0.2987, -0....    0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000])\n",
      "target_class_ids = tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "pred_bbox = tensor([[ 0.3104, -0.0039,  0.5234,  0.2698],\n",
      "        [-0.6332, -0.5304,  0.0569, -0.1449],\n",
      "        [ 0.0336,  0.1035,...      [ 0.1433, -0.7981,  0.4616, -0.3784],\n",
      "        [-0.0305, -0.1762,  0.0261,  0.2933]], grad_fn=<AsStridedBackward>)\n",
      "\n",
      "    @saveall\n",
      "    def mrcnn_bbox(target_bbox, target_class_ids, pred_bbox):\n",
      "        \"\"\"Loss for Mask R-CNN bounding box refinement.\n",
      "    \n",
      "        target_bbox: [batch, num_rois, (dy, dx, log(dh), log(dw))]\n",
      "        target_class_ids: [batch, num_rois]. Integer class IDs.\n",
      "        pred_bbox: [batch, num_rois, num_classes, (dy, dx, log(dh), log(dw))]\n",
      "        \"\"\"\n",
      "        # todo review what is zeropadded and where\n",
      "    \n",
      "        # todo bs>1. need to do all the below for each batch OR vectorize\n",
      "        target_bbox, target_class_ids, pred_bbox = unbatch(target_bbox, target_class_ids, pred_bbox)\n",
      "    \n",
      "        if len(target_class_ids):\n",
      "            # Only positive ROIs contribute to the loss. And only\n",
      "            # the right class_id of each ROI. Get their indicies.\n",
      "            positive_roi_ix = torch.nonzero(target_class_ids > 0)[:, 0]\n",
      "            positive_roi_class_ids = target_class_ids[positive_roi_ix].long()\n",
      "            indices = torch.stack((positive_roi_ix, positive_roi_class_ids), dim=1)\n",
      "    \n",
      "            # Gather the deltas (predicted and true) that contribute to loss\n",
      ">           target_bbox = target_bbox[indices[:, 0], :]\n",
      "E           IndexError: too many indices for tensor of dimension 1\n",
      "\n",
      "../../maskmm/loss.py:110: IndexError\n",
      "______________________________ test_mrcnn_mask[0] ______________________________\n",
      "\n",
      "t = <maskmm.baseline.Test object at 0x7f2ba806d860>, index = 0\n",
      "\n",
      "    @pytest.mark.parametrize(\"index\", range(4))\n",
      "    def test_mrcnn_mask(t, index):\n",
      ">       t.run(model.compute_mrcnn_mask_loss, loss.mrcnn_mask)\n",
      "\n",
      "../../test/test_loss.py:25: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "../../maskmm/baseline.py:196: in run\n",
      "    self.results = self.func(*self.inputs)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "target_masks = tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.]...       [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "target_class_ids = tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "pred_masks = tensor([[[0.8670, 0.8628, 0.8490,  ..., 0.9628, 0.9732, 0.9860],\n",
      "         [0.8618, 0.8113, 0.8375,  ..., 0.9341, 0.968...0.5049, 0.5299],\n",
      "         [0.4585, 0.5176, 0.4752,  ..., 0.5672, 0.4549, 0.5500]]],\n",
      "       grad_fn=<AsStridedBackward>)\n",
      "\n",
      "    @saveall\n",
      "    def mrcnn_mask(target_masks, target_class_ids, pred_masks):\n",
      "        \"\"\"Mask binary cross-entropy loss for the masks head.\n",
      "    \n",
      "        target_masks: [batch, num_rois, height, width].\n",
      "            A float32 tensor of values 0 or 1. Uses zero padding to fill numpy.\n",
      "        target_class_ids: [batch, num_rois]. Integer class IDs. Zero padded.\n",
      "        pred_masks: [batch, proposals, height, width, num_classes] float32 tensor\n",
      "                    with values from 0 to 1.\n",
      "        \"\"\"\n",
      "        # todo bs>1. need to do all the below for each batch OR vectorize\n",
      "    \n",
      "        target_masks, target_class_ids, pred_masks = unbatch(target_masks, target_class_ids, pred_masks)\n",
      "    \n",
      "        if len(target_class_ids):\n",
      "            # Only positive ROIs contribute to the loss. And only\n",
      "            # the class specific mask of each ROI.\n",
      "            positive_ix = torch.nonzero(target_class_ids > 0)[:, 0]\n",
      "            positive_class_ids = target_class_ids[positive_ix].long()\n",
      "    \n",
      "            indices = torch.stack((positive_ix, positive_class_ids), dim=1)\n",
      "    \n",
      "            # Gather the masks (predicted and true) that contribute to loss\n",
      ">           y_true = target_masks[indices[:, 0], :, :]\n",
      "E           IndexError: too many indices for tensor of dimension 2\n",
      "\n",
      "../../maskmm/loss.py:144: IndexError\n",
      "______________________________ test_mrcnn_mask[1] ______________________________\n",
      "\n",
      "t = <maskmm.baseline.Test object at 0x7f2ba8073fd0>, index = 1\n",
      "\n",
      "    @pytest.mark.parametrize(\"index\", range(4))\n",
      "    def test_mrcnn_mask(t, index):\n",
      ">       t.run(model.compute_mrcnn_mask_loss, loss.mrcnn_mask)\n",
      "\n",
      "../../test/test_loss.py:25: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "../../maskmm/baseline.py:196: in run\n",
      "    self.results = self.func(*self.inputs)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "target_masks = tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.]...       [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "target_class_ids = tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "pred_masks = tensor([[[0.8670, 0.8628, 0.8490,  ..., 0.9628, 0.9732, 0.9860],\n",
      "         [0.8618, 0.8113, 0.8375,  ..., 0.9341, 0.968...0.5049, 0.5299],\n",
      "         [0.4585, 0.5176, 0.4752,  ..., 0.5672, 0.4549, 0.5500]]],\n",
      "       grad_fn=<AsStridedBackward>)\n",
      "\n",
      "    @saveall\n",
      "    def mrcnn_mask(target_masks, target_class_ids, pred_masks):\n",
      "        \"\"\"Mask binary cross-entropy loss for the masks head.\n",
      "    \n",
      "        target_masks: [batch, num_rois, height, width].\n",
      "            A float32 tensor of values 0 or 1. Uses zero padding to fill numpy.\n",
      "        target_class_ids: [batch, num_rois]. Integer class IDs. Zero padded.\n",
      "        pred_masks: [batch, proposals, height, width, num_classes] float32 tensor\n",
      "                    with values from 0 to 1.\n",
      "        \"\"\"\n",
      "        # todo bs>1. need to do all the below for each batch OR vectorize\n",
      "    \n",
      "        target_masks, target_class_ids, pred_masks = unbatch(target_masks, target_class_ids, pred_masks)\n",
      "    \n",
      "        if len(target_class_ids):\n",
      "            # Only positive ROIs contribute to the loss. And only\n",
      "            # the class specific mask of each ROI.\n",
      "            positive_ix = torch.nonzero(target_class_ids > 0)[:, 0]\n",
      "            positive_class_ids = target_class_ids[positive_ix].long()\n",
      "    \n",
      "            indices = torch.stack((positive_ix, positive_class_ids), dim=1)\n",
      "    \n",
      "            # Gather the masks (predicted and true) that contribute to loss\n",
      ">           y_true = target_masks[indices[:, 0], :, :]\n",
      "E           IndexError: too many indices for tensor of dimension 2\n",
      "\n",
      "../../maskmm/loss.py:144: IndexError\n",
      "______________________________ test_mrcnn_mask[2] ______________________________\n",
      "\n",
      "t = <maskmm.baseline.Test object at 0x7f2ba8089550>, index = 2\n",
      "\n",
      "    @pytest.mark.parametrize(\"index\", range(4))\n",
      "    def test_mrcnn_mask(t, index):\n",
      ">       t.run(model.compute_mrcnn_mask_loss, loss.mrcnn_mask)\n",
      "\n",
      "../../test/test_loss.py:25: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "../../maskmm/baseline.py:196: in run\n",
      "    self.results = self.func(*self.inputs)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "target_masks = tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.]...       [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "target_class_ids = tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "pred_masks = tensor([[[0.8670, 0.8628, 0.8490,  ..., 0.9628, 0.9732, 0.9860],\n",
      "         [0.8618, 0.8113, 0.8375,  ..., 0.9341, 0.968...0.5049, 0.5299],\n",
      "         [0.4585, 0.5176, 0.4752,  ..., 0.5672, 0.4549, 0.5500]]],\n",
      "       grad_fn=<AsStridedBackward>)\n",
      "\n",
      "    @saveall\n",
      "    def mrcnn_mask(target_masks, target_class_ids, pred_masks):\n",
      "        \"\"\"Mask binary cross-entropy loss for the masks head.\n",
      "    \n",
      "        target_masks: [batch, num_rois, height, width].\n",
      "            A float32 tensor of values 0 or 1. Uses zero padding to fill numpy.\n",
      "        target_class_ids: [batch, num_rois]. Integer class IDs. Zero padded.\n",
      "        pred_masks: [batch, proposals, height, width, num_classes] float32 tensor\n",
      "                    with values from 0 to 1.\n",
      "        \"\"\"\n",
      "        # todo bs>1. need to do all the below for each batch OR vectorize\n",
      "    \n",
      "        target_masks, target_class_ids, pred_masks = unbatch(target_masks, target_class_ids, pred_masks)\n",
      "    \n",
      "        if len(target_class_ids):\n",
      "            # Only positive ROIs contribute to the loss. And only\n",
      "            # the class specific mask of each ROI.\n",
      "            positive_ix = torch.nonzero(target_class_ids > 0)[:, 0]\n",
      "            positive_class_ids = target_class_ids[positive_ix].long()\n",
      "    \n",
      "            indices = torch.stack((positive_ix, positive_class_ids), dim=1)\n",
      "    \n",
      "            # Gather the masks (predicted and true) that contribute to loss\n",
      ">           y_true = target_masks[indices[:, 0], :, :]\n",
      "E           IndexError: too many indices for tensor of dimension 2\n",
      "\n",
      "../../maskmm/loss.py:144: IndexError\n",
      "______________________________ test_mrcnn_mask[3] ______________________________\n",
      "\n",
      "t = <maskmm.baseline.Test object at 0x7f2ba8205e10>, index = 3\n",
      "\n",
      "    @pytest.mark.parametrize(\"index\", range(4))\n",
      "    def test_mrcnn_mask(t, index):\n",
      ">       t.run(model.compute_mrcnn_mask_loss, loss.mrcnn_mask)\n",
      "\n",
      "../../test/test_loss.py:25: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "../../maskmm/baseline.py:196: in run\n",
      "    self.results = self.func(*self.inputs)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "target_masks = tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.]...       [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "target_class_ids = tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)\n",
      "pred_masks = tensor([[[0.8670, 0.8628, 0.8490,  ..., 0.9628, 0.9732, 0.9860],\n",
      "         [0.8618, 0.8113, 0.8375,  ..., 0.9341, 0.968...0.5049, 0.5299],\n",
      "         [0.4585, 0.5176, 0.4752,  ..., 0.5672, 0.4549, 0.5500]]],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       grad_fn=<AsStridedBackward>)\n",
      "\n",
      "    @saveall\n",
      "    def mrcnn_mask(target_masks, target_class_ids, pred_masks):\n",
      "        \"\"\"Mask binary cross-entropy loss for the masks head.\n",
      "    \n",
      "        target_masks: [batch, num_rois, height, width].\n",
      "            A float32 tensor of values 0 or 1. Uses zero padding to fill numpy.\n",
      "        target_class_ids: [batch, num_rois]. Integer class IDs. Zero padded.\n",
      "        pred_masks: [batch, proposals, height, width, num_classes] float32 tensor\n",
      "                    with values from 0 to 1.\n",
      "        \"\"\"\n",
      "        # todo bs>1. need to do all the below for each batch OR vectorize\n",
      "    \n",
      "        target_masks, target_class_ids, pred_masks = unbatch(target_masks, target_class_ids, pred_masks)\n",
      "    \n",
      "        if len(target_class_ids):\n",
      "            # Only positive ROIs contribute to the loss. And only\n",
      "            # the class specific mask of each ROI.\n",
      "            positive_ix = torch.nonzero(target_class_ids > 0)[:, 0]\n",
      "            positive_class_ids = target_class_ids[positive_ix].long()\n",
      "    \n",
      "            indices = torch.stack((positive_ix, positive_class_ids), dim=1)\n",
      "    \n",
      "            # Gather the masks (predicted and true) that contribute to loss\n",
      ">           y_true = target_masks[indices[:, 0], :, :]\n",
      "E           IndexError: too many indices for tensor of dimension 2\n",
      "\n",
      "../../maskmm/loss.py:144: IndexError\n",
      "===================== 16 failed, 21 passed in 3.85 seconds =====================\n",
      "time: 3.89 s\n"
     ]
    }
   ],
   "source": [
    "class InferenceConfig(Config):\n",
    "    pass\n",
    "pytest_args = [\"/home/ubuntu/maskmm/test\"]\n",
    "#log.setLevel(logging.WARNING)\n",
    "pytest.main(pytest_args)\n",
    "log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
